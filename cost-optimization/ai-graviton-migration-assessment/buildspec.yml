version: 0.2

phases:
  install:
    runtime-versions:
      python: 3.11
      nodejs: 22
    commands:
      - echo "Installing dependencies..."
      - node --version && npm --version
      - curl -fsSL https://desktop-release.transform.us-east-1.api.aws/install.sh | bash
      - export PATH="/root/.local/bin:$PATH"
      - atx --version
      - pip install boto3

  pre_build:
    commands:
      - export PATH="/root/.local/bin:$PATH"
      - echo "Repository URL - $REPOSITORY_URL"
      - echo "Output Bucket - $OUTPUT_BUCKET"
      - echo "Job ID - $JOB_ID"
      - echo "Downloading fresh AWS Graviton reference data..."
      # Download AWS Porting Advisor for compatibility rules
      - |
        if wget -q -O porting-advisor.tar.gz https://github.com/aws/porting-advisor-for-graviton/archive/main.tar.gz; then
          echo "✓ Downloaded latest Porting Advisor rules from GitHub"
          tar -xzf porting-advisor.tar.gz
          mkdir -p ./reference-data
          cp -r porting-advisor-for-graviton-main/src/advisor/rules/ ./reference-data/
          cp -r porting-advisor-for-graviton-main/src/advisor/constants/ ./reference-data/
          echo "✓ Porting Advisor compatibility rules prepared"
        else
          echo "⚠ Porting Advisor download failed, using embedded rules"
          mkdir -p ./reference-data
        fi
      # Download AWS Graviton Getting Started guide for performance and best practices
      - |
        if wget -q -O graviton-guide.tar.gz https://github.com/aws/aws-graviton-getting-started/archive/main.tar.gz; then
          echo "✓ Downloaded latest Graviton Getting Started guide from GitHub"
          tar -xzf graviton-guide.tar.gz
          mkdir -p ./reference-data/performance ./reference-data/services
          echo "✓ Graviton performance and service guidance prepared"
        else
          echo "⚠ Graviton guide download failed, using embedded guidance"
        fi
      # Extract and structure the fresh reference data
      - python3 -c "
import os
import json
import re
from pathlib import Path

def extract_software_versions():
    '''Extract software version requirements from Graviton Getting Started'''
    versions = {}
    try:
        readme_path = 'aws-graviton-getting-started-main/README.md'
        if os.path.exists(readme_path):
            with open(readme_path, 'r') as f:
                content = f.read()
            
            # Extract software version table
            table_match = re.search(r'Package\s+Version\s+Improvements.*?\n\n', content, re.DOTALL)
            if table_match:
                table_content = table_match.group(0)
                lines = table_content.split('\n')[3:-2]  # Skip headers and footer
                
                for line in lines:
                    if '|' in line:
                        parts = [p.strip() for p in line.split('|') if p.strip()]
                        if len(parts) >= 3:
                            package = parts[0]
                            version = parts[1]
                            improvement = parts[2]
                            versions[package] = {
                                'min_version': version,
                                'improvements': improvement
                            }
            
            # Save structured data
            os.makedirs('./reference-data/performance', exist_ok=True)
            with open('./reference-data/performance/software-versions.json', 'w') as f:
                json.dump(versions, f, indent=2)
            print(f'✓ Extracted {len(versions)} software version recommendations')
    except Exception as e:
        print(f'⚠ Software version extraction failed: {e}')

def extract_compiler_flags():
    '''Extract compiler optimization flags'''
    flags = {}
    try:
        readme_path = 'aws-graviton-getting-started-main/README.md'
        if os.path.exists(readme_path):
            with open(readme_path, 'r') as f:
                content = f.read()
            
            # Extract processor table with mcpu flags
            table_match = re.search(r'Processor.*?Graviton4.*?\n\n', content, re.DOTALL)
            if table_match:
                table_content = table_match.group(0)
                
                # Extract mcpu recommendations
                mcpu_match = re.search(r'Recommended -mcpu flag.*?\n(.*?)\n', table_content, re.DOTALL)
                if mcpu_match:
                    flags['graviton2'] = {'mcpu': 'neoverse-n1'}
                    flags['graviton3'] = {'mcpu': 'neoverse-512tvb'}
                    flags['graviton4'] = {'mcpu': 'neoverse-512tvb'}
            
            # Save structured data
            with open('./reference-data/performance/compiler-flags.json', 'w') as f:
                json.dump(flags, f, indent=2)
            print(f'✓ Extracted compiler optimization flags')
    except Exception as e:
        print(f'⚠ Compiler flags extraction failed: {e}')

def extract_service_patterns():
    '''Extract AWS service-specific patterns'''
    patterns = {
        'containers': {
            'supported': True,
            'services': ['Docker', 'Kubernetes', 'Amazon ECS', 'Amazon EKS'],
            'registry': 'Amazon ECR supports multi-arch containers'
        },
        'lambda': {
            'supported': True,
            'benefits': 'Up to 34% better price performance',
            'pricing': '20% lower duration charges',
            'compute_savings_plans': True
        },
        'databases': {
            'rds_postgresql': 'Available with performance improvements',
            'considerations': 'Check PostgreSQL optimization flags'
        }
    }
    
    try:
        # Save structured service patterns
        os.makedirs('./reference-data/services', exist_ok=True)
        with open('./reference-data/services/aws-services.json', 'w') as f:
            json.dump(patterns, f, indent=2)
        print('✓ Extracted AWS service patterns')
    except Exception as e:
        print(f'⚠ Service patterns extraction failed: {e}')

# Execute extractions
extract_software_versions()
extract_compiler_flags()
extract_service_patterns()
print('✓ Fresh reference data extraction completed')
"
      - echo "Creating Graviton migration assessment configuration..."
      # Download custom transformation definition from S3
      - aws s3 cp s3://$OUTPUT_BUCKET/graviton-transformation-definition/ ./transformation-definition/ --recursive
      # Download knowledge items from S3
      - aws s3 cp s3://$OUTPUT_BUCKET/knowledge-items/ ./knowledge-items/ --recursive
      # Create configuration file for custom transformation
      - |
        cat > transform-config.yaml << EOF
        transformationDefinitionPath: "./transformation-definition"
        codeRepositoryPath: "."
        buildCommand: "npm run build || mvn compile || pip install -r requirements.txt || echo 'No build command available'"
        referenceDataPath: "./reference-data"
        knowledgeItemsPath: "./knowledge-items"
        EOF
      - echo "Cloning target repository..."
      - git clone --depth 1 $REPOSITORY_URL ./target-repo
      - ls -la ./target-repo

  build:
    commands:
      - export PATH="/root/.local/bin:$PATH"
      - echo "Running custom Graviton migration transformation..."
      # Change to target-repo and run custom transformation
      # NOTE: cd persists across commands in CodeBuild, affecting post_build too
      - cd target-repo
      # First publish the transformation definition, then execute it
      - atx custom def publish -n graviton-migration-assessment --description "Graviton migration assessment transformation" --sd ../transformation-definition
      - atx custom def exec -n graviton-migration-assessment -p "." -x -t -d
      - echo "Custom Graviton transformation completed"
      - echo "Validating generated artifacts..."
      - |
        if [ -d "Assessment" ]; then
          echo "✓ Assessment reports generated"
          find Assessment -name "*.md" | head -5
        fi
        if [ -d "Migration-Artifacts" ]; then
          echo "✓ Migration artifacts generated"
          find Migration-Artifacts -type f | head -10
        fi
      - echo "Generating enhanced cost analysis..."
      - |
        python3 -c "
        import boto3
        import json
        try:
            pricing = boto3.client('pricing', region_name='us-east-1')
            print('✓ Enhanced cost analysis with real-time pricing available')
            # Add basic instance type cost lookups
        except Exception as e:
            print(f'⚠ Cost analysis using static estimates: {e}')
        " || echo "Using static cost estimates"
      - ls -la

  post_build:
    commands:
      - export PATH="/root/.local/bin:$PATH"
      - echo "Uploading Graviton assessment and migration artifacts to S3..."
      # We're still in target-repo directory from build phase
      # Upload both assessment reports and migration artifacts
      - |
        if [ -d "Assessment" ]; then
          aws s3 cp Assessment/ s3://$OUTPUT_BUCKET/assessments/$JOB_ID/Assessment/ --recursive
          echo "✓ Assessment reports uploaded"
        fi
        if [ -d "Migration-Artifacts" ]; then
          aws s3 cp Migration-Artifacts/ s3://$OUTPUT_BUCKET/assessments/$JOB_ID/Migration-Artifacts/ --recursive
          echo "✓ Migration artifacts uploaded"
        fi
        # Upload any additional generated files
        find . -name "*.md" -not -path "./Assessment/*" -not -path "./Migration-Artifacts/*" -exec aws s3 cp {} s3://$OUTPUT_BUCKET/assessments/$JOB_ID/ \;
      - echo "Graviton assessment and artifacts uploaded to s3://$OUTPUT_BUCKET/assessments/$JOB_ID/"